## Data Analytics

We can define data analytics as the process of collecting, cleaning, organizing, analyzing, and interpreting data to uncover insights and make informed decisions.


### Data Analytics vs. Data Science

As you continue to learn about data, you will come across tons of data-related terminology. Among these are two often-confused terms: data analysts and data scientists. These two roles are similar in the sense that they both work with data to gather insights, but how they work with data is what sets them apart. In this reading, you will learn the differences between these two disciplines by reviewing their roles, responsibilities, skills, and backgrounds. 

**Data Analysts**

Data analysts work with structured data to identify patterns, build visualizations, and extract meaningful insights that help organizations make informed decisions.  

**Responsibilities**

Data analysts are typically responsible for maintaining databases, interpreting data sets, and creating reports that effectively present data trends, patterns, and predictions. Some common tasks include gathering data from various sources, cleaning and organizing data, and presenting findings in easy-to-understand visualizations.

**Skills and Tools**

Foundational mathematics and statistics 

Analytical thinking and data visualization

Basic fluency in R, Python, and SQL 

SAS, Excel, and business intelligence software

**Background**

Data analysts are commonly experienced in mathematics and statistics. They might also have a degree in mathematics, statistics, computer science, or finance. 

**Data Scientists**

Data scientists work with various data types, including structured and unstructured data. They use advanced data techniques, including machine learning and predictive modeling, to design processes, develop models, and extract insights from data. 

Responsibilities

Data scientists are typically responsible for arranging undefined datasets, writing algorithms, building automation systems, and statistical models. Some of their common tasks include gathering and cleaning raw data, creating data visualization tools, dashboards, and reports, and developing code to automate data collection and processing. 

**Skills and Tools**

Advanced statistics and predictive analytics

Machine learning and data modeling

High-level, object-oriented programming

Hadoop, MySQL, TensorFlow, and Spark

Background 

Data scientists are commonly experienced in computer science and are generally required to have a master’s or doctoral degree in data science, information technology, mathematics, or statistics. 

Although their titles are similar, data analysts and data scientists have distinct roles, requirements, and career paths. Now that you know the difference between the two, consider this as you continue your journey in data analysis. 

Conclusion

Although these two disciplines are very similar and often go hand-in-hand in terms of skills and how they work with data, there are some subtle differences to keep in mind as you explore which focus you’d like to pursue in how you work with data.


### OSEMN Framework Overview
 
 
**Obtain:** Gather the data   
•	Determine what data would be useful  
•	Evaluate what data are available  
•	Decide on how the data can be gathered  
 
**Scrub:** Clean the data to prepare it for analysis  
•	Correct inconsistent formatting  
•	Remove duplicate records  
•	Handle missing values  
•	Remove inaccurate information  
 
**Explore:** Search for interesting patterns and statistics that stand out  
•	Examine variable distributions  
•	Examine variable relationships  
•	Perform statistical tests  
 
**Model:** Generate predictions and insights  
•	Select a model type for your goals (often in cooperation with a partner)  
•	Categories of models include:  
o	Classification - Is this “A” or “B”?  
o	Regression - How much or how many?  
o	Clustering - What natural segments can we find in our data?  
 
**iNterpret:** Help others to understand the results of your analysis  
•	Build visualizations  
•	Construct stories  
•	Create presentations of your findings  


### Obtaining Data


#### Sampled Data

 Sampled data is data from a subset of a larger population or a larger data set that's used to represent the entire population or data. In other words, you are a smaller number of data, that's a good representation of the total data set you would like to study. 
 
 It's a common practice in data analytics to sample data, because analyzing the entire population can be costly, time consuming, or even impossible. Sampling allows us to draw conclusions about the population while only analyzing a fraction of it. 
 
 Let's run through some situations where sampled data is necessary. First, the population might be too large. When dealing with large data sets or populations, analyzing the entire data set can be impractical or impossible. 
 
 Imagine you are a market researcher and you work for a large auto manufacturer, the company wants to get a better understanding of their customer satisfaction. They would like to know how satisfied their car buyers are with the performance of the cars, as well as with the service they're getting from the dealerships. 
 
 You know, you can get all this information using a survey in which you ask customers questions about their experience and have them rate how satisfied they are. But because your company has **hundreds of thousands** of customers, it's not practical to send a survey to each individual who owns one of your cars. Instead, you decide to send a survey to a sample of your customer base, a smaller group of customers that you will select as a representation of the larger customer population. 
 
 Second, and related to the first point, you might have **cost constraints**, collecting data from an entire population can be expensive. Sampling can be more **cost effective**, allowing researchers to allocate resources to other important tasks. In our example, for the car company, surveying everyone would cost a lot of money just in getting the surveys out to everyone. But it would cost you even more to get responses from all these customers. You might need to use incentives to get customers to answer, which could cost a lot of money if you're talking to hundreds of thousands of people. 
 
 You could also face **time constraints**, collecting data from an entire population can also be time consuming. Sampling can save time and allow researchers to collect and analyze data quickly. For our car company, collecting responses from a very large group of people would take you quite a bit more time than focusing on a smaller number of customers who bought a car from your company.
 
 In some cases, it might not be possible to analyze the entire population because it would be destroyed in the process. Data analysts refer to this as **destructive sampling**. For example, testing every product manufactured in a candy factory would result in the destruction of the entire inventory and the destruction of your teeth, but that's not really what's referred to here. 
 
 So for all of these reasons, you might be dealing with a subset of the data instead of the whole data set, or in other words, a sample. But when you work with sample data, there are a few things you should consider to help you evaluate the quality of your data. 
 
 First, the sample size. Sample size is the number of observations in the sample. A larger sample size generally provides more accurate estimates of the population. If the sample size is small, it might not give you enough to work with, and the data might not accurately represent the population. 
 
 A smaller sample will also limit the types of analyses you can perform on your data, because certain analyses require many observations to provide an accurate result. 
 
 In other words, if you want a reliable analysis, larger samples are usually a must. The sample should also be representative of the population being studied. If the sample is biased, the conclusions drawn from the sample might not be valid for the entire population. 
 
 Let's say you are conducting a study on the average salary of employees in a certain company. The company has a total of 1000 employees, and you decide to survey 50 of them to collect data on their salaries. However, you only choose to survey employees who work in the finance department because you assume that their salaries will be representative of the entire company. In this scenario, the sample is not representative of the entire population of the company because it only includes employees from one department. 
 
 The sample is biased towards finance employees and might not be accurate for the entire company. This can lead to incorrect conclusions being drawn from the data, such as assuming that the entire company has a higher average salary than it actually does. 
 
 To ensure representativeness, it would be better to use a more random sampling method that includes employees from different departments and positions. Such a sample would better represent the entire company, and it will give you a more accurate read on the average salary. 
 
 Another important thing to think about when using samples is the generalizability of your sample. The conclusions drawn from the sample might not be applicable to other populations. It's important to be aware of the limitations of the sample and not over generalize the findings. 
 
 Let's say you are conducting a study on the eating habits of college students in a particular university. You decide to collect data by surveying 100 randomly selected students who attend to university. However, your study only includes students who attend that specific university, which means that the conclusions drawn from the study might not be generalizable to all college students or to the population at large. 
 
 This is because the students attending that university might have different demographics, cultural backgrounds, and socioeconomic statuses compared to students at other universities. For example, the university in question might be located in an urban area and attract more students from low income households. This means that the study's findings might not be applicable to students attending universities in suburban or rural areas or students from higher income households. Therefore, it's important to consider the generalizability of the sample when conducting research to avoid drawing incorrect conclusions or making incorrect generalizations about a larger population patient.
 
**Conclusion:** Sampled data is a useful tool in data analytics when dealing with large populations or data sets. However, it's important to consider the sample size, representativeness, and generalizability when working with sampled data to ensure that the conclusions drawn are valid.

#### First-Party Data

**Definition:**
First-party data refers to information collected directly by a company or organization from its own audience, such as customers, website visitors, or app users. This data is considered highly valuable because it is unique to the company and typically involves direct interactions with the brand.

**How It Is Collected:**

- **Website and App Analytics:** Data collected from user interactions on a company's website or mobile app, such as page views, time spent on pages, click-through rates, and purchase history.
- **Customer Relationship Management (CRM) Systems:** Information gathered from customer interactions, including contact details, purchase history, and customer service interactions.
- **Surveys and Feedback Forms:** Direct input from customers through surveys, feedback forms, and user reviews.
- **Email Marketing Campaigns:** Data collected from email open rates, click-through rates, and responses to email campaigns.
- **Loyalty Programs:** Information from customer participation in loyalty and rewards programs.

**Who Collects It:**
- **Businesses:** Retailers, e-commerce platforms, service providers, and any organization with a direct relationship with customers.
- **Organizations:** Non-profits, educational institutions, and government agencies collecting data from their members or service users.

#### Third-Party Data

**Definition:**
Third-party data refers to information collected by entities that do not have a direct relationship with the users from whom the data is derived. This data is aggregated from various sources and sold to other companies for marketing and analytical purposes.

**How It Is Collected:**
- **Data Aggregators:** Companies that specialize in collecting data from multiple sources, such as websites, apps, public records, and social media platforms.
- **Purchase and Usage Data:** Information gathered from third-party apps and services that users interact with.
- **Cookies and Tracking Pixels:** Data collected through cookies and tracking technologies placed on various websites to monitor user behavior across the web.
- **Public Records and Social Media:** Information pulled from public databases and social media platforms where users share data publicly.
- **Surveys and Panels:** Data collected from third-party surveys and consumer panels, where participants opt-in to share their information.

**Who Collects It:**
- **Data Brokers:** Companies like Acxiom, Experian, and Nielsen that gather, compile, and sell data to other businesses.
- **Advertising Networks:** Ad networks like Google AdSense and Facebook Audience Network collect data to target advertisements more effectively.
- **Market Research Firms:** Organizations that conduct market research and compile data from various sources to provide insights to businesses.

#### Key Differences

1. **Source of Data:**
   - **First-Party:** Directly from the company's own customers or users.
   - **Third-Party:** Indirectly from external sources without a direct relationship with the data subjects.

2. **Data Quality:**
   - **First-Party:** Typically more accurate and relevant, as it comes from direct interactions.
   - **Third-Party:** Can be broad and extensive but might lack accuracy and specificity due to aggregation from various sources.

3. **Control and Privacy:**
   - **First-Party:** Companies have full control over data collection and usage, often leading to better compliance with privacy regulations.
   - **Third-Party:** Companies must rely on the data provider's compliance with privacy laws, which can introduce risks.

4. **Usage:**
   - **First-Party:** Primarily used for enhancing customer experience, personalization, and direct marketing.
   - **Third-Party:** Used for broadening customer insights, expanding reach, and enhancing advertising targeting.


#### An Overview of Helpful Free Datasources

Accessing data is simpler than ever, and there is a wide range of helpful data sources at your disposal. Here's a list of free data sources to help you gather information and insights more effectively, along with links to those resources. 

[Google Public Dataset Search](www.datasetsearch.research.google.com)

Like Google Scholar, Google Dataset Search provides access to millions of datasets hosted on public websites, such as Kaggle and OGD Platform India, in thousands of locations on the internet.


[United States Census Bureau](www.census.gov)

The United States Census Bureau provides access to quality and essential data about the United States’ population, economy, and geography.


[Pew Research Center](https://www.pewresearch.org/tools-and-resources/)

The Pew Research Center provides insights and analysis on a wide range of social, political, and technological issues through surveys and research.


[Eurostat](www.ec.europa.eu/eurostat)

As the European Union's statistical office, Eurostat provides comprehensive economic, social, and environmental data.


[The Organization for Economic Co-Operation and Development (OECD)](www.data.oecd.org/united-states.htm)

The OECD is a reliable source for comparative data and analysis on global economic and social matters.


[Kaggle Datasets](www.kaggle.com/datasets)

Kaggle hosts hundreds of thousands of high-quality public datasets from several industries to explore, analyze, and share. 



[National Centers for Environmental Information (NCEI)](www.ncei.noaa.gov)

NCEI is part of NOAA’s Office of Oceanic and Atmospheric Research and provides environmental data regarding climate change and global chemical measurements. 


[World Bank Open Data](www.data.worldbank.org)

This comprehensive dataset includes indicators such as population size and unemployment rates collected from hundreds of countries worldwide, offering insights into global economic, social, and environmental trends.



#### Summary: Validity of Data

When obtaining data, it is important to check the validity of your dataset, or in other words, ensuring your data are of high quality so you can move on to the explore and analyze phase. 

Here is a checklist you can use to ensure the validity of your data 

**Source credibility:**

⏹ Authorship: Is the data provided by a reputable author or organization? What are the credentials of the author or organization?

⏹ Publication date: Is the data current and up-to-date?

**Methodology:**

⏹ Sample size: Was the data collected from a large enough sample?

⏹ Sampling method: Was the sampling method unbiased and representative?

⏹ Data collection: Were the data collection methods clearly described and appropriate?

**Objectivity:**

⏹ Bias: Are there any apparent biases in the data or its presentation?

⏹ Conflicts of interest: Are there any potential conflicts of interest that could influence the data?

**Accuracy:**

⏹ Consistency: Are the data consistent with other reputable sources?

⏹ Error rate: Are there any obvious errors or inconsistencies in the data?

**Relevance:**

⏹ Scope: Is the data relevant to the research question or topic?

⏹ Context: Is the data presented within a meaningful context?


### Scrub the Data

Clean your data to ensure that it is usable for the next phases where you will explore, model, and interpret your data. 

What does it mean to scrub your data and why is it so important? 

Scrubbing data is sometimes also referred to as cleaning data. The scrubbing face transforms raw, dirty data into clean data. 

The scrubbing process can be divided into four main tasks:

1. Removing duplicates,  
2. Formatting records,   
3. Solving for missing values,   
4. Checking records for mistakes or wrong values.   

Dirty data is any data that contains duplicate records, is inconsistently formatted, has missing values, or contains inaccurate information. In contrast, clean data contains only unique records, has a consistent structure, has no missing values and contains reliable and accurate information. Ensuring your data is clean is an essential step before analyzing the data for further insights. 

Without the scrubbing stage, the errors in your data might affect your exploration and analysis, and they can lead you to drawing the wrong conclusions. Or in some cases, they can mess up any analysis you want to do on the data or make any model you apply to your data gets stuck, making it impossible to draw any conclusions at all.

#### Scrubbing Checklist

The scrubbing stage is all about cleaning your data and getting your dataset ready for analysis.  You can use this checklist to help you in the process.

1. Removing Duplicates

⏹ Identifying duplicate records: inspect records for duplicates and verify that they are actually a duplicate record. 

⏹ Remove duplicate records: remove the duplicate records from your dataset

2. Formatting records**

⏹ Ensure consistency: check all data follow a consistent format and adjust the format if necessary

3. Solving for missing values

⏹ Identify the missing values

⏹ Solve for the missing values: Replace the missing values with text (e.g. NA) or delete the entire record with the missing value

4. Checking for wrong values

⏹ Identify wrong values

⏹ Solve for the wrong values: Replace the wrong values with the correct ones if you can or delete the entire record with the wrong values


### Exploring Data

#### Visualize data

Anscombe's quarter:

4 sets of data with same mean and standard deviation but all has different distribution. It is important to visualize data to really understand the relationships.

#### Data Distributions

1. Normal Distribution

2. Log-normal Distribution

3. Exponential Distribution


#### Data relationships

**Correlations**

** Causations**


#### Summary: Exploring Data

Explore Checklist

What is your data telling you?

⏹ Inspect your data: If your dataset isn’t too large, read through your data to assess whether interesting information jumps out

⏹ Use summary statistics: Evaluate your data by summarizing it (categorize, use statistics like average, standard deviation, etc.)

⏹ Inspect a random sample of your data: if your dataset is too large, a random sample may give you some initial information

Visualizing data

⏹ Visualize your data using bar charts, line charts or scatter plots to examine information hidden in your dataset. 

Bar charts		Line charts		Scatter plots

Examine variable distributions

⏹ Inspect the distribution of your data 

Categorize the data 

Plot the categorized data

Common data distributions:

Normal	Bimodal		Log-normal 		Exponential		Uniform

Learn more about your data:

⏹ Evaluate the minimum

⏹ Evaluate the maximum

⏹ Evaluate the mode

⏹ Evaluate the standard deviation

Examine variable relationships

⏹ Visualize variables to understand their correlation

Common visualizations: 

Scatter plot 				Line chart

⏹ Calculate the correlation coefficient to understand the strength of the correlation

0  = no correlation

1   = perfect positive correlation

-1 = perfect negative correlation

Feature engineering

⏹ Evaluate whether we can create new features or modify existing ones to better understand our data



### Modeling

We've obtained the data we needed, scrubbed it, and explored it thoroughly. Now, we're ready for stage 4 "Modeling". This phase is about using our data to make predictions with mathematical models. These models can be anything from simple linear regressions to advanced machine learning algorithms depending on the project. Although there are many different models, they all work by discovering hidden patterns in data and using it to make predictions on any new data we give the model. For example, you might build a model to predict how many conversions you expect a campaign to deliver. You would do that by using data from the past to predict the future. Our discussion of modeling is going to be broken down into the following 3 sections: What are models, how the models work, and the types of models? All of the steps of the OSEMN process are important, but modeling is a central piece of data analysis. 


'' All models are wrong but some are useful'' Statistician Box


#### Real world Example

In previous videos, we followed Keira, a data analysts working with Carlos, the owner of Inu and Neko, a dog and cat care company. Carlos had approached Keira to help and launch a new subscription meal service for cats and dogs and wanted to select the 10 best products to offer as part of the subscription. Keira started by obtaining the necessary data from their e-commerce software. She downloaded last year's sales data into Google Sheets to analyze which products were most popular and purchased repeatedly. She obtained the data properly by making sure it was credible, collected accurately, objective, accurate, and relevant to Carlos' questions. Keira proceeded to scrub the dataset, checking for duplicates, inconsistent formatting in the zip codes, and missing values like phone numbers and sales totals. She standardized zip codes for consistency and remove the phone number column because it was not crucial for analysis. Keira filled in missing sales stores by calculating product price multiplied by quantity. Some records lacked customer IDs, preventing the assessment of repeat buyers. So Keira deemed them unhelpful and removed them from the dataset. Lastly, she identified inaccurate information such as negative sales amounts and unusually high prices, recognizing them as glitches in Inu+Neko’s system and excluded them from consideration to maintain data accuracy for future analysis. Now that Keira has completed scrubbing the data successfully while addressing duplicates, inconsistencies, proportionate gaps, and inaccuracies, she's ready to dive into our next topic; explore and model. In the explore and model stages of the OSEMN Framework, Kiera is now tasked with uncovering patterns and trends within the data and creating a model to predict subscription bundle preferences for Inu and Neko's customers. During the explorer stage, Keira will perform various exploratory data analysis techniques to gain insights. This might involve using charts and visualizations to identify correlations between different product categories or demographic segments. She'll also dive deeper into customer behavior by analyzing purchase frequency, average basket sizes, and seasonal trends. Once Keira has explored the data thoroughly. She can move on to modeling. Modeling involves using algorithms to create a predictive model based on historical data. Keira will develop models to help Carlos meet his goal of hitting 500 subscriptions. In the upcoming videos, we'll follow Keira through the explore and model stages of the OSEMN Framework! Let's get started!


#### Exploring Data

Kira now has a clean set of data that she considers relevant for the question she got from Carlos at Inu+Neko: Which products should he include in the new dog and cat food subscription product to help him reach 500 subscribers by the end of the year? Kira gets started with Step 3 of the OSEMN cycle, she starts to explore the data. She thinks that a good place to start is to see what types of data are in each column. She sees that she has both categorical and numerical data. There's a date column and text columns like order numbers and customer IDs. She notices that the order number and customer ID columns are very long and take up a lot of screen space and are hard to type. So, she decides to map each unique value to a number in a process called encoding. Encoding is the process of turning a string of data into numerical data by mapping each unique string to a unique number. Encoding can be used to reduce the amount of memory needed to work with data, or to make large, complex strings easier to work with. Can also be used to turn text data into numbers when a model needs numerical inputs instead of text inputs. In this example, Kira's using encoding to make the data easier to view and understand. She also notices that there are quite a few columns that contain redundant information. A column is redundant if you could look at one column and always guess what's in the other column. In this case, the customer ID and name are redundant. They might not always be, but for this product she knows it's safe to assume, so she hides customer name. She also notices that SKU and product name are redundant as well, so she hides the SKU. She also decides that it is unlikely that this project will look at data at the street address level, so she only keeps the state and zip code. Great, now she can see all of the useful columns at a glance. And not having a large text columns even helps her computer run more smoothly. Next, she decided to run some summary statistics for the remaining columns. She looks at things like the most common value, how many times it appears, and what percentage of the rows it shows up in. She also looks at the minimum, maximum, range, and mean of the numerical values. From this, she sees that the data spans 899 days from the spring of 2019 through the summer of 2021. She sees that Texas is the most popular state, which makes sense because it is quite populous. There are also some numerical columns that she thinks could come in handy, like price and quantity. Kira decides to add up the total sales for each product and use a bar chart to look at the distribution of these total sales. This gives her exactly what she was looking for, the top selling products. She can also see from this chart that cat products tend to sell more than dog products. She realizes that it's great to know what top selling products are, but it might also be important to know what causes those products to sell more. To answer that question, she creates a few scatter plots to observe the relationships between the quantities sold and different variables. One relationship that stands out to her is quantity sold and price. She notices that if she looks at just cat or dog products in isolation, the quantity sold is low for low and high-priced items, but high for items in the middle. This means that the two variables have a positive correlation for low prices, and a negative correlation for high values. This is great information to pass on to Carlos. Maybe adding more medium-priced items will help him find additional products for his subscription service. Kira now knows the top selling products and a possible reason for why those products are the top sellers. Kira's ready to move on to the next stage in her analysis, the model stage.


#### Type of data

1. Ordinal Data

Ordinal data refers to a type of data that involves order or rank. Unlike nominal data, which is purely categorical and lacks any inherent order, ordinal data has a meaningful order among its categories, but the intervals between the categories are not necessarily consistent or known.

An example of ordinal data is a survey response scale for customer satisfaction:

- Very Unsatisfied 
- Unsatisfied  
- Neutral   
- Satisfied   
- Very Satisfied  

In this example, the responses have a clear order (from very unsatisfied to very satisfied), but the difference between "Very Unsatisfied" and "Unsatisfied" is not necessarily the same as the difference between "Neutral" and "Satisfied." This makes it ordinal because while the ranking is meaningful, the exact differences between ranks are not quantified.


### Interpreting the Results

The interpret stage is where you interpret your analysis. It's arguably the most important. Without it, all we would have is data and statistics. 

The interprets stage translates your analytical findings back to a business context. After successful modeling stage, you'll have a new tool like a regression model that can be used to generate predictions. 

The answers generated from these sorts of models are very specific and usually aren't immediately interpretable or understandable by non-technical team members. 

During the interprets stage, our goal is to close the loop of the OSEMN cycle by using the models and insights we generated during the exploration and modeling phases to try and answer the business question driving the entire project. 

In other words here you look back at your objective for your analysis. Your goal here is twofold. 

First and foremost you need to understand the results of your model and all the insights it can provide. These might be the actual predictions the model makes, like forecasting the results of sales from a campaign or they might be information contained within your model like an insight that shows that mailing lists sign-ups are strong predictors of people spending more money with your company.

Second, you need to be able to explain your findings to a non-technical audience in a clear concise way. Simply understanding the implications of your model isn't enough. You need to be able to make others understand it and trust your results. 

Remember, analytics projects are about generating **actionable insights** or information that can be used to make better decisions that help the company.

- What was the objective of this analysis? 

It's important to go back to your starting point because that will remind you of the questions you set out to answer. It's quite easy to get lost in the data during the model and explore stages and lose sight of your initial question. 

- How does the data answer my questions? 

Maybe the data shows you that the business goal you set is currently unattainable, or maybe it gives you a plan that you could use to move forward. 

- What other learnings do I have? 

In the process of answering one business question, you will often find new pieces of potentially useful information that help solve the problem at hand in a different way. Or maybe they open up new potential business objectives you can address in later analysis. 

- How can I apply this to a business context? 

Gaining new knowledge is great, but it is important to focus on information that's actionable and moves your business forward in meaningful ways. It will often be someone else that takes action based on your information, so think about how that will happen. 

- How confident should I be in my results? 

If you see an improvement in a business metric, was it due to the changes you made or was it due to random chance? Many data analysts are overconfident in their results, and when they implement what they have learned, they quickly discover that something was wrong with their analysis. That brings us to the topic of, how do you know if you should be confident in the results of your model? 

Earlier, during the modeling stage, we briefly discussed using a separate set of test data to check your trained model against. The testing process is all about ensuring you have the right amount of confidence in your model. By running your test data through your model, you can answer questions like, how wrong is the model on average? If the model predicts something, how likely is it to be correct or incorrect? Are there particular scenarios that cause the model to be incorrect? 

Even the best models have limitations, so it's important to know what they are. On top of those basic questions, you can also use a tool called **statistical testing** to quantify how confident you should be. 

Statistical tests are mathematical methods of ensuring that differences are not caused by random chance. Sometimes this is called the significance of the results. 

For example, you're trying to improve an email campaign. Your model recommends a change that you implement, and you see a 5% increase in sales. Great, you just made the company 5% more money, right? Well, not so fast, it's possible that the increase in sales was random or because of some other factor. How can you know? You might run a statistical test and see that you should be 80% confident that the change in revenue was due to your new improved emails. It's then up to you or your organization to decide how confident you need to be to take action. 

Sometimes organizations want to be between 90 and 95% confident to take action, other times, they're happy with greater than 50%. It often depends on how risky it is for your business to be wrong. 

This is why statistical tests are so useful to businesses. So how do statistical tests work? To be honest, there's a lot of complicated math involved that we won't get into here. But there are some things they generally measure, including the differences in the averages of the datasets. 

If the averages are very different, the difference is less likely to be caused by randomness, the size of the dataset. The more data you have, the more confident you should be that the difference in averages isn't random, even if it's small. And the distributions of the datasets, this is often measured using standard deviation. 

A high standard deviation indicates high variability or that data values on average fall far from the mean. And a low standard deviation would mean that data in general is closer to the mean. If your data sets have high standard deviations, even large differences in their averages can simply be due to randomness. It's important to note here that none of these metrics in isolation provide a quantifiable measure of confidence. Only by combining them using statistical tests can you measure confidence.

The interpret phase of the awesome framework is crucial because it's where data driven insights are evaluated and communicated. You must revisit your initial analysis objectives, understand how the data answers your questions, and uncover any additional findings. Moreover, it's vital to ensure these insights are actionable within your business context. 

Tools like statistical testing play a key role, helping quantify your confidence in the results. And ultimately, the goal of the interpret phase and data analysis overall is not just to gain insights, but to make informed decisions that propel your business forward.



**the interprets stage of the OSEMN process**

This is where all the data explorations and number crunching finally pay off. This is also where we explain our findings and generate concrete recommendations for our organizations. Now that we've explored the entire OSEMN process to understand our model, we want to make the hard numbers tell a story. It needs to be a story that anyone in our team, not just the data experts can understand and act upon. This is a crucial step because it's not just about having insights, it's about communicating them effectively. 

An essential aspect of effective data storytelling is choosing the right medium to communicate your findings, and there are many different mediums you could use. Some examples could be a slide presentation or an interactive notebook, or an in-depth report for an executive review. Each of these comes with its own strengths and challenges, and being able to adapt your story-telling approach to fit each medium is an important skill in data analytics. 

Slide presentations are universally relevant across industries and job roles and provide a structured, visual and engaging way to take your audience through your findings and recommendations. Your presentation should recap the original goal, review how you went through the steps of the OSEMN cycle, visualize critical data points, and importantly, explain your findings and recommendations. 

**Key components of a slide presentation**

1. Original problem

Start your presentation by taking your audience back to the original problem that initiated your analysis. Re-introduce the issue at hand. What were we trying to solve? Why did we consider it necessary to undertake this analysis? Highlight the potential implications of not addressing this issue, illustrating why it was significant enough to warrant such an in-depth investigation. The purpose of this segment is to establish the context, helping your audience comprehend the relevance of the forthcoming findings and recommendations. 

2. The Method

Now that you've established the context, take the audience through the method you used. In this example you're going to take the audience through the steps of the OSEMN process that was followed. You want to maintain high-level overview, provide enough context for each step so the audience can understand the methods that lead to the insights. Remember, the goal here isn't to dive into highly technical details. You simply want to build a clear picture of the process leading up to your findings. If you're presenting to a more technical audience though, you might want to include more of the technical details. Now, guide your audience through a visual tour of the data, using aesthetically appealing and easy to understand visuals such as graphs, charts, and tables. You should try to encapsulate the core data points that lead to your findings. Make sure these visuals are accessible to a broad audience and designed in a way that even non data analysts can comprehend. The role of visualizations it's not just to display data, but to make the data speak for itself. Highlighting trends, anomalies, patterns, and correlations that underscore your findings. With visuals presented, it's time to describe them for your audience. This is where you act as the translator, decoding the visuals and turning data into a narrative. Elaborate on the key observations drawn from the data, explaining what they signify in the context of the original problem. Try to link the patterns and trends into visuals to the story you're trying to tell. Make sure your explanations are straightforward and relatable so that the audience can easily understand the story coming from the data. As you approach the end of your presentation, present your recommendations based on the findings, what actions should be taken to address the problem identified at the beginning of the presentation. If your data reveals a potential for improvement or modification in certain areas, outline those recommendations clearly. Discuss why you believe these steps would make a difference. Drawing connections between your findings and the recommended actions.


Let's imagine we've been analyzing a recent decline in web traffic for an e-commerce store. We start our presentation with a recap. 

We remind the audience about the initial problem; a significant drop in website traffic over the past three months, which we noticed during our routine metrics review. 

Next, reshare the OSEMN process. We obtained website analytics data, scrubbed it to ensure accuracy, then explore that to identify trends and anomalies. 

Our exploration led us to the modeling phase where we created a model that identifies potential costs. 

We then present a visualization, a graph illustrating the downward trend in website traffic alongside the increase in page load time, which we discovered was the primary cause for our model output. The clear inverse correlation visually substantiates our key finding. We can use the graph to explain what happens and when the problem started. As the page load time increased, the web traffic decreased significantly. We're able to tell the story of how our users started leaving our website because of longer low tides causing the website traffic to decline. And finally, we conclude with our recommendation. Given the correlation between page load time and traffic, we suggest optimizing the website's performance in an effort to reduce low time, which should help recover and possibly increase our website traffic. Thus, our data journey comes full circle, offering actionable insights that can be used to improve our business. Interpreting your findings and explaining them effectively to your audience is what turns data into action. It's the crucial final step in the OSEMN cycle, one that bridges the gap between raw data and real-world decisions, and that's the power of data analytics


**Explain, Enlighten and Engage**

After you've analyzed your data and drawing your conclusions, you will often need to present your findings. You'll want to make sure that you can convey your findings well and persuade people to understand and believe that. In order to tell a persuasive story, we need to focus on the three Es; explain, enlighten, and engage. 

This can be achieved by a combination of data, narratives and visuals. Both the data and a narrative will serve to explain the situation. The narrative provides context for the data, specifically, where it comes from, why stakeholders should care about it and what was done with it. Data and visualization are part of enlightening your audience, as we've mentioned before, raw data are simply a bunch of values. It's hard for most people to appreciate what the data I have to say in their raw form. By combining the data with good visuals, one can show clearly what the data are and what they mean to the overall big picture. You want to lead your audience to that aha moment to point where everything clicks. Engagement comes from the narrative and visualizations. If the visualizations are good and the narrative is clear and concise, people can internalize what's being said. This internalization gets them invested in the story. As the Venn diagram shows all three parts together, the data visualizations and narrative can lead to audience understanding and persuasion as they're brought together using the three Es of explain, enlighten, and engage. That's frequently the goal of the story.
Play video starting at :1:59 and follow transcript1:59
Suppose you're presenting data on decreasing honeybee populations. You have data on the global decline in honeybee populations over the last decade gathered from various environmental research agencies. This data includes annual honeybee colony counts across several countries. You prepare a story around the importance of honeybees in the equal system, highlighting their role in pollinating a majority of the food crops we consume. You discuss the potential consequences of their declining numbers and why it matters to everyone, not just environmentalists. You create a line graph that vividly illustrates the decrease in honeybee populations over time across different countries. This visual representation allows for a clearer understanding of the magnitude of the problem. The numbers alone would illustrate. Now, how do these elements combine to explain, enlightened, and engage? You use the narrative to clarify the data's origin and its significance. Stating disfigures derived from multiple environmental agencies showed the alarming rate of decline in honeybee populations over the past 10 years. The visual of the line graph is used to illustrate the data. You'd see this graph powerfully depicts the downward trend allowing us to visualize the severity of the issue at hand. Finally, you combine the narrative and visuals saying, imagine a future where many fruits and vegetables becomes scarce due to the lack of pollination. This graph isn't just lines and dips. It represents potential problems to our food supplies that might need to be addressed. This makes the story memorable and drives home the urgency of the issue. In future videos, we'll go deeper into data storytelling and how to tell an effective and compelling story. For now, remember that a good data story persuades your audience by transforming the data into stories that explain, enlighten, and engage.

So far, we focused quite a bit on data and we also saw how to create compelling visuals. But how about a narrative? How can we build a good narrative to help people understand and be persuaded by the data and the visuals? Every compelling story, from novels to movies to data analysis, typically has four key parts. Setup, buildup, climax, and conclusion. To create an impact, your data story should incorporate these elements. Let's look at them one by one. As with any good story, we should start our story with a hook. Something to get the audience interested in following along. Frequently, these hooks are questions derived from curiosity. Is there a sudden change? Are we missing an opportunity? What should we expect moving forward? What we want to convey in the setup is the theme of the story. It could be that there is an issue or concern that needs to be addressed, or an opportunity to be seized. For our example hook, suppose that we notice a sudden dip in Inu and Neku's sales for the last couple of months. An obvious and compelling hook is why? What could be causing this downturn? After we've set up the story, we want to create a build up. Build up is where the story unfolds. It's here that we describe the steps taken in investigating the hook from the setup. We also want to communicate the findings from our investigations. The actions taken that lead us to the key findings are particularly important. The key finding is the insight from our analysis that has the greatest explanatory power. In our example, after some data exploration, we realized that the sales figures are from multiple channels. They can be broken up into Internet, wholesale, and retail. It looks like the change in sales is stemming from our Internet sales. We then investigated the web data and found that while Internet sales numbers went down, the number of customers visiting the online store did not. Digging further, we discovered that for Internet customers, there was a high abandonment rate of shopping carts, meaning many customers never checked out even though they had items in their cart. This appears to have started at the same time as the downturn started. So that would be our key finding. Online shopping cart abandonment rates increased during the downturn. If this was a mystery story, the climax happens when the villain is unmasked. For us, it's when we explain the hook's root cause with our key finding. Ideally, this is where the audience's light bulb goes off. If you engaged with the audience adequately, they should now understand the dynamic between the hook and the cause and want to act on your insight. In our story, it's strange that 92% of customers abandoned their cards while in the process of making a purchase. This didn't happen in prior months or years. It's at this point that we uncover the fact that customers abandoned their cards due to our lack of inventory for the products that they want to buy. If we don't have the product in stock, of course our customers can't buy it. Consequently, we received no sales for those products. Now we finish the story. If there is action that needs to be taken to remedy the issue, it should be revealed at this point. We should also discuss the cause if we have an idea of what it is. Going back to our example with Inu and Neku, now that we've uncovered the cause for the decline in sales, how do we fix it? Well, if we work to increase our stock of in demand products, we should be able to reverse the downward sales trend. If the products were in stock, our sales would have likely looked much better and the decline wouldn't have happened.

Should be mentioned that you don't always need to tell a nice and neat story when interpreting data. But the most impactful and meaningful discoveries tend to. When the insights are hard to understand or when the impact on your business is large, storytelling with data is essential. Expect an underlying story when data shows something unexpected, unpleasant, complex, costly, or especially surprising. These situations tend to point to a good data story waiting to be told.


Summary Reading: iNterpreting Data & Storytelling
iNterpret Checklist 

Step 1: Understand the results of your analysis

Ask the following questions: 

⏹ What was the objective for this analysis?

⏹ How does the data answer my questions?

⏹ What other learnings do I have?

⏹ How can I apply this to a business context?

⏹ How confident should I be?

How wrong is the model?

How likely is the model to be correct?

What scenarios cause the model to be incorrect?

Step 2: Explain your findings

Build a presentation with these key components:

⏹ Recap

⏹ Method

⏹ Visualization

⏹ Explanation

⏹ Recommendation

